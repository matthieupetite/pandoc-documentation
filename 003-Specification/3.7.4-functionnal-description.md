# Datascience functionnal description

## Stackeholders

The Data Platform encompasses a wide array of stakeholders, each playing a crucial role in its operation and success. The primary stakeholders include Data Producers, Data Scientists, DevOps Teams, and Data Consumers.

Data Producers can be either real persons or other systems deployed across the April Organization. They are responsible for generating and providing raw data from various sources, ensuring that the data is accurate, reliable, and timely. This data forms the backbone of the platform, feeding into various applications and analyses.

Data Scientists utilize the data provided by the producers to perform complex analyses, develop models, and extract insights that drive informed decision-making. Their activities include data cleaning, statistical analysis, machine learning, and data visualization, all aimed at deriving actionable intelligence from the available data.

DevOps Teams are tasked with maintaining the infrastructure that supports the Data Platform. They ensure the platform's reliability, scalability, and performance through continuous integration, deployment, and monitoring practices. Their activities are critical in maintaining a seamless operation and enabling the efficient functioning of the platform.

<!-- OK si on etait en mode 1 du coup, mais dans notre cas la pluspart des taches presentes ici sur l'equipe devops vont se retrouver sur l'equipe Data Science Group du coup non ? Les taches de l'equipe DevOps s'arrentent selon moi au maintient des zones cores sur ce use case.-->

Data Consumers are the end-users who leverage the processed data and insights to make strategic decisions, improve operations, and drive business value. They may include business analysts, executives, and other stakeholders within the organization, who rely on the data to inform their actions and strategies.

<!-- Et on a aussi des sytemes interne en consumer non ? via les API des custom apps -->

## Data Platform Support Components

They are two main Data Platform Support components:

- The Data Governance
- The Data Quality

### Data Governance

<!-- Pas hors scopes ces 2 points ? Data Gov/Qual -->

**Data Governance** is a critical support component that involves the management of data availability, usability, integrity, and security in the Data Platform. This function ensures that data is consistently accurate, reliable, and accessible to authorized users. Data Governance encompasses policies, procedures, and standards that govern data collection, storage, and usage, thereby maintaining compliance with regulatory requirements and organizational objectives. Key activities include defining data ownership, establishing data stewardship roles, and implementing data policies that align with business objectives.

The selected tool for implementing the data governance process is DataHub. DataHub is a powerful open-source metadata platform designed to enable data discovery, data observability, and data governance. This versatile tool centralizes metadata from various sources, providing a unified view of the data landscape. By leveraging DataHub, organizations can easily track data lineage, monitor data quality, and enforce data governance policies.

DataHub's capabilities make it an excellent choice for implementing data governance functions. Its extensible architecture supports a wide range of data sources and integrates seamlessly with existing data workflows. This flexibility ensures that DataHub can adapt to the evolving needs of the organization. Additionally, its user-friendly interface and robust security features make it accessible to both technical and non-technical users, fostering a culture of data stewardship and accountability throughout the organization.

By employing DataHub, organizations can ensure that their data governance processes are not only efficient but also scalable, thereby supporting the long-term goals of data integrity and reliability.

### Data Quality

**Data Quality** is another essential support component focused on maintaining high standards for data accuracy, completeness, consistency, and timeliness. This function includes processes and tools designed to identify, prevent, and correct data issues, ensuring that the data used by the platform and other systems is of the highest quality. Data Quality management involves data profiling, data cleansing, and monitoring processes that help detect anomalies and inconsistencies in data sets. By maintaining high-quality data, organizations can enhance decision-making processes and drive better business outcomes.

Both Data Governance and Data Quality systems are key parts of the Data Platform, they are mainly deployed to support the data platform needs but are also designed to bring functionalities to other systems within April System. These functions provide a robust framework for ensuring data integrity and reliability across various platforms, thereby supporting the overarching goals of the organization.

## Data Platform Technical Components

Next to the Data Platform Support components, some technical components are required to runs and monitor all software included in the platform. Those components are:

- Compute
- Storage
- Monitoring
- Shared components

### Compute

In this category we can find the following azure objects:

- Virtual Machines: Provision and manage computing resources.
- Containers: Lightweight, portable, and consistent computing environments.
- Serverless Functions: Execute code in response to events with automatic scaling.

The main choice to meet the Data Platform requirements is to provide computes with the Azure Kubernetes Solution (AKS).

The AKS solution offers several advantages that make it an excellent choice for the Data Platform Landing Zone. First, its ability to manage containerized applications significantly simplifies deployment, scaling, and operations by leveraging Kubernetes orchestration. This ensures high availability and fault tolerance, which are critical for any data platform.

Furthermore, AKS provides seamless integration with other Azure services, such as Azure Entra ID for identity management and Azure Monitor for comprehensive monitoring and diagnostics. This integration ensures that the data platform is secure, compliant, and well-monitored. AKS's serverless Kubernetes approach also means that resources are allocated dynamically, optimizing costs and performance without manual intervention(In our private environment, via Autoscaling Node Pools). 

Additionally, the flexibility of AKS allows for the deployment of a wide range of workloads, from microservices to large-scale data processing tasks, thus meeting diverse computational needs. Its compatibility with CI/CD pipelines ensures that updates and deployments are conducted efficiently and reliably.

Finaly, it allow us to leverage editor provided HELM charts to provision out of the shelf customisable production ready middleware solutions (Dagster deployment for example).

### Storage

In this category we can find the following azure objects:

- Databases: Structured data storage with SQL or NoSQL models.
- Data Lakes: Centralized repository for storing large amounts of raw data.
- Object Storage: Store unstructured data such as images, videos, and backups.

It is important to note that these storage solutions are not typically used for storing business data. Instead, they are utilized to store data used by applications deployed in Azure Kubernetes Service (AKS), such as configuration data or process data.

### Monitoring

- Application Performance Monitoring (APM): Track application performance and detect issues.
- Infrastructure Monitoring: Observe and analyze the performance of infrastructure components.
- Log Management: Collect, analyze, and store log data from various sources.

We aim to use Application Insights to meticulously observe the behavior and performance of Python applications deployed in Azure Kubernetes Service (AKS). This powerful tool will help us monitor application health, detect anomalies, and diagnose issues effectively.

Furthermore, we plan to employ Prometheus to gather critical insights on the AKS cluster itself. Prometheus, renowned for its robust monitoring and alerting toolkit, is adept at collecting and storing metrics, allowing us to visualize and analyze the performance and health of our Kubernetes environment comprehensively. Its query language, PromQL, enables precise and flexible querying of time-series data, making it indispensable for maintaining the clusterâ€™s efficiency.

To complement this, we will leverage Grafana to render the collected data across various dedicated reports. Grafana is a versatile dashboard and visualization tool that integrates seamlessly with Prometheus, enabling us to create intuitive and customizable dashboards. This integration will allow us to visualize trends, track performance metrics, and share insights across the organization, thereby facilitating data-driven decision-making.

In summary, Prometheus and Grafana are the right tools for our data platform because they offer a scalable, flexible, and user-friendly approach to monitoring and visualization. Their combined capabilities ensure that we maintain optimal performance and reliability across our applications and infrastructure.

For log management, we will utilize Azure Monitor, storing logs in a dedicated Log Analytics workspace. This setup will ensure that we have centralized, scalable, and secure log storage, enabling efficient querying and analysis. For further details on our monitoring strategy and infrastructure, please refer to the Data Platform Team Monitoring chapter, which comprehensively outlines the methodologies and tools employed.

### Shared Components

This section includes all Azure components that will be utilized to support the data platform component from a networking and security perspective. Those components are for example:

- API Gateway: Manage and secure API calls between microservices.
- Message Queues: Enable asynchronous communication between different parts of the system.
- Authentication Services: Manage user authentication and authorization.
- Key vault : store secrets and certificates used in the platform.

## Data Platform Core Components

### Ingest

#### Data sources

Data sources in a data platform refer to the origins of the data that is ingested, stored, and processed within the system. These sources can be diverse, encompassing structured data from databases, unstructured data from logs or social media, and semi-structured data from JSON or XML files. In the ingestion phase, data is collected from these varied sources and integrated into the platform for subsequent processing and analysis.

In the context of April, the data sources are primarily hosted on Snowflake, a cloud-based data warehousing platform known for its scalability and performance. Snowflake enables seamless integration of various data types, ensuring efficient ingestion and storage. This robust infrastructure supports the platform's needs for high-volume data processing and real-time analytics. Some data source can be hosted in other location such as on premises databases, but it is not the main use case. That explain why the data platform must have access to the on-premises datacenter through the connectivity landing zone.


<!-- Ces sources donnÃ©es sont out of scope ATM, pour ne pas faire doublon avec la plateforme ETL Talend existante et standard -->
Other identified data sources at this stage of the project are AS400, Oracle, MySql databases on premises, file shares on NAS, and eventually SharePoint. By integrating these additional sources, the platform can ensure a comprehensive and diverse data repository, enhancing the overall data processing and analytics capabilities.

#### Data Extraction

The data extraction process in a data platform is the initial step of the data lifecycle, where raw data is gathered from its origin points and transferred into the system for further manipulation. In this phase, various techniques are employed to ensure the smooth and efficient extraction of data from disparate sources.

The extraction process typically begins with identifying and accessing the relevant data sources, which may include cloud-based solutions like Snowflake, on-premises databases, APIs, IoT devices, and more. Connectivity to these sources is established using secure methods to maintain the integrity and confidentiality of the data.

Once the connections are in place, different extraction methods are utilized based on the nature of the data and the source systems. For structured data, SQL queries or data replication tools might be used, while unstructured data may require web scraping, API calls, or log file parsing. Semi-structured data, such as JSON or XML, often involves specialized parsers to handle the nested structures and extract the relevant information.

During extraction, it's crucial to manage data consistency and quality. Techniques such as incremental extraction, where only the changes since the last extraction are captured, help in reducing the load on source systems and minimizing the data transfer volume. Data validation rules may also be applied to ensure that the extracted data meets the required standards before moving to the next phase.

The extracted data is then staged in an operational storage area within the data platform, where it undergoes preliminary processing. This includes tasks like data cleaning, normalization, and transformation to ensure it is in a suitable format for further processing.

### Transform

#### Data versionning

Data versioning in a data platform refers to the practice of keeping track of changes and iterations of datasets over time. This allows data engineers and analysts to maintain a historical record of data, making it possible to track how data has evolved, replicate results, and ensure data integrity. By implementing data versioning, teams can manage and recover previous versions of datasets, facilitating auditing, debugging, and compliance with regulatory standards. Furthermore, it enables collaborative work, as multiple users can work on data without overwriting each other's changes.

It is a crucial feature of Aprilâ€™s data platform, where we aim to version datasets stored as files. For this purpose, we utilize tools such as Data Version Control (DVC) or Git Large File Storage (Git LFS). These tools allow us to efficiently track changes in large datasets, ensuring that every modification is documented and recoverable. DVC provides a systematic approach to versioning data by integrating seamlessly with Git, while Git LFS handles the storage of large files, enabling scalable and effective data management.

<!-- Out of scope aussi pour le moment -->
#### Model Management

Model management in a data platform involves the systematic handling, deployment, and monitoring of machine learning models. It ensures that models are versioned, reproducible, and can be seamlessly integrated into production environments. This process encompasses activities such as tracking model performance, managing model versions, and automating the deployment process to improve reliability and scalability.

We are targeting the Databricks solution for our model management needs. Databricks offers a unified analytics platform that simplifies the end-to-end workflow for machine learning. Its robust infrastructure supports collaborative workspaces, scalable compute resources, and seamless integration with various data sources, making it an ideal choice for our requirements.

Databricks integrates seamlessly with Microsoft Azure, offering a powerful combination of Databricks' unified analytics platform and Azure's robust cloud services. Azure Databricks provides a collaborative and scalable environment where data scientists, data engineers, and business analysts can work together on big data analytics and machine learning projects.

The integration allows users to leverage Azure's security, compliance, and flexibility features while taking advantage of Databricks' capabilities. Users can easily provision Azure Databricks through the Azure portal, and it integrates natively with other Azure services such as Azure Data Lake Storage, Azure SQL Database, and Azure Machine Learning. This tight integration simplifies data ingestion, storage, and processing, enabling organizations to build and deploy machine learning models more efficiently.

Databricks also supports seamless integration with Azure Active Directory, ensuring secure and streamlined access management. Furthermore, Azure Databricks provides a fully managed Apache Spark environment, which allows users to perform large-scale data processing and advanced analytics without worrying about infrastructure management.

#### Data Testing

Data Testing involves the continuous observation and evaluation of data as it flows through various systems and processes. This practice ensures data quality, integrity, and compliance by detecting anomalies, validating data against predefined rules, and providing alerts for any deviations. Implementing robust data testing is essential for maintaining trust in data-driven decisions and overall operational efficiency.

To achieve this, we will utilize the Great Expectations tool. Great Expectations is an open-source platform designed for data validation, documenting, and profiling. It allows us to define expectations or rules for our data, enabling automated checks to ensure that our data meets the desired standards. By integrating Great Expectations with our Azure Databricks PaaS, we can seamlessly monitor data quality in real-time, ensuring that any issues are promptly identified and addressed.

#### Data Orchestration

Data orchestration is the process of coordinating and managing the flow of data through various systems, ensuring that data is correctly ingested, processed, and delivered to the right destinations. It involves integrating multiple data sources, transforming data as needed, and executing workflows that facilitate seamless data movement across an organization's ecosystem. This holistic approach helps to streamline data operations, improve data quality, and enhance the efficiency of data-centric processes.

To implement effective data orchestration, we have chosen to use Dagster. Dagster is an open-source orchestrator that excels in managing complex data pipelines. It allows us to define, schedule, and monitor workflows with precision, making it easier to ensure that data tasks are executed in the correct order and dependencies are managed appropriately. Dagster's modular architecture and compatibility with various tools and frameworks make it an ideal choice for our needs.

The main reason that conducts April to choose Dagster as the main tool for data orchestration are the following:

- **Modular Design**: Dagster's modularity enables us to break down our data workflows into manageable, reusable components. This design philosophy promotes scalability and maintainability.
- **Integrated Data Validation**: With built-in support for data validation, Dagster ensures that data quality checks are seamlessly integrated into the orchestration process, reducing the risk of errors and improving data reliability.
- **Observability**: Dagster provides robust monitoring and logging capabilities, offering real-time insights into the state of our data pipelines. This observability is crucial for promptly identifying and addressing any issues.
- **Flexibility**: Dagster's flexibility allows us to integrate it with our existing tools and platforms, such as Azure Databricks. This compatibility ensures a smooth transition and maximizes our current investments.
- **Community and Support**: As an open-source project with an active community, Dagster benefits from continuous improvements and shared best practices. This community support enhances our ability to leverage the latest advancements in data orchestration.

By leveraging Dagster, we aim to achieve a robust and efficient data orchestration framework that supports our organization's data-driven goals.

#### Operational Data Storage

Operational Data Storage in a data platform refers to the system or infrastructure used to store, manage, and retrieve operational data that is generated and used during the daily operations of an organization. This data is often characterized by its high volume, velocity, and variety, requiring a resilient and scalable solution to handle it effectively.

April has chosen to use Snowflake as the solution for Operational Data Storage. Snowflake offers a cloud-based data warehousing platform that provides seamless scalability, high performance, and robust security features. Its architecture allows for efficient data storage and fast query performance, enabling the organization to manage its operational data with greater flexibility and reliability. By opting for Snowflake, April ensures that the data platform can meet the demands of modern data-driven applications and analytics. 

In addition to its technical advantages, April's decision to adopt Snowflake was also motivated by a strategic goal to streamline the technology stack within the data platform. By consolidating operational data storage into a single, comprehensive solution, the organization can reduce the complexity involved in managing multiple technologies. This consolidation minimizes integration challenges, simplifies maintenance, and optimizes resource allocation, ensuring a more efficient and cohesive data management environment.

#### Data Transformation

Data transformation is a crucial process within a data platform that involves converting raw data into a more structured and usable format. This process typically includes cleaning, enriching, and structuring data to ensure it is accurate, consistent, and accessible for analysis. Data transformation is essential because it prepares the data for downstream applications such as analytics, reporting, and machine learning models.

One of the core objectives of data transformation is to establish a single source of truth by refining raw data through various stages, often referred to as the bronze, silver, and gold operational data levels. At the bronze level, raw data is ingested into the platform with minimal transformation, serving as a comprehensive repository of all incoming data. As data progresses to the silver level, it undergoes further cleaning and enrichment, ensuring it is standardized, deduplicated, and integrated. Finally, at the gold level, data is fully transformed and optimized for specific business use cases, providing high-quality, analytics-ready datasets.

This multi-tiered approach not only enhances data quality and reliability but also ensures that the data is appropriately refined to meet the needs of diverse applications and stakeholders within the organization. By transforming raw data through these stages, the data platform can deliver accurate and actionable insights, supporting informed decision-making and strategic initiatives.

April has selected dbt (data build tool) as the primary solution for implementing the data transformation process. dbt is an open-source tool that enables data analysts and engineers to transform data within the data warehouse by writing simple select statements. dbt handles the complexity of data transformation, including managing dependencies and ensuring data integrity, making it an ideal choice for modern data platforms.
The key reasons for choosing dbt include:

- **Version Control and Collaboration**: dbt allows teams to version control their transformation code and collaborate efficiently, ensuring that changes are tracked and documented.
- **Modularity and Reusability**: dbt encourages modular design, enabling the reuse of transformation logic across different projects and datasets, thereby reducing redundancy and improving maintainability.
- **Testability**: dbt supports automated testing of transformation logic, ensuring data quality and consistency throughout the transformation process.
- **Extensibility**: dbt integrates seamlessly with modern data warehouses, including Snowflake, and can be extended with custom functions and macros to meet specific transformation needs.

By adopting dbt, April can streamline its data transformation processes, enhance collaboration among data teams, and ensure high-quality data for its data-driven applications and analytics.

#### Data lake

One of the core components of a modern data platform is the data lake. A data lake serves as a centralized repository that allows the storage of all structured and unstructured data at any scale. This capability is vital for organizations looking to implement a robust data strategy. The data lake becomes the single source of truth, retaining vast amounts of historical data for long-term storage and retrieval. Itâ€™s the final destination of refined data in the Data Transformation processes.

By leveraging a data lake, April can maintain the integrity and accessibility of its data, thus ensuring that valuable information is preserved over time. This long-term retention is critical for compliance, auditing, and analytical purposes. The flexibility of a data lake allows it to accommodate diverse data types and formats, making it an indispensable part of any comprehensive data platform.

Moreover, the data stored in the lake can be processed and analyzed to derive insights that drive decision-making and innovation. With all data housed in one location, data teams can efficiently manage, query, and transform data, ensuring consistent and accurate information across all applications.

April has chosen to employ the Snowflake data lake solution as part of its modern data platform strategy. This decision is motivated by several compelling factors. Snowflake's solution offers a robust and scalable environment that seamlessly integrates with existing data infrastructure, reducing the complexity and the number of distinct technologies involved in managing and analyzing data. By consolidating various data management functions into a single, unified platform, April can streamline its data operations, leading to increased efficiency and reduced operational overhead.

Snowflake's architecture is designed to handle both structured and unstructured data at any scale, aligning perfectly with April's needs for a versatile and comprehensive data storage solution. Additionally, the platform's advanced security features ensure that data integrity and compliance requirements are met, providing peace of mind for long-term data retention and audit purposes. The ability to easily query and transform data within Snowflake further enhances April's capacity for deriving actionable insights and driving innovation.
This strategic decision to use Snowflake not only simplifies April's technological ecosystem but also positions the company to leverage cutting-edge data technologies to stay ahead in a competitive market.

#### Data Applications

One of the key aspects of the April data platform is its ability to support various data applications, which are essential tools and functionalities designed to process, analyze, and interpret data efficiently. These applications leverage the robust capabilities of the platform to deliver sophisticated solutions tailored to specific business needs.

For instance, data applications can include the Juniper Playbook, which is a strategic tool used for planning and executing business strategies based on data-driven insights. Another example is data transformation and enrichment APIs that allow for the seamless conversion and enhancement of raw data into valuable information. These APIs can perform tasks such as data cleaning, normalization, and integration from multiple sources, thus ensuring that data is consistent and ready for analysis.

April has chosen to develop these data applications using Python, given that the primary contributors are data scientists who are proficient in this programming language. The applications are hosted in Azure Kubernetes Service (AKS) as pods, which ensures scalability and efficient resource management. Most of these applications are implemented as Python Flask solutions, a lightweight and flexible web framework that facilitates the development and deployment of web applications.

### Visualize

#### Data Visualization

An integral part of a data platform is data visualization, which transforms raw data into meaningful insights through visual representations such as charts, graphs, and dashboards. For April, the preferred tool to implement this functionality is QlikSense. This choice is driven by its existing deployment within the organization and the proficiency of April's data analysts with the tool.

QlikSense leverages data qualified as the source of truth to generate comprehensive and sharable reports. These reports are crucial for decision-making processes across all levels of the organization, ensuring that stakeholders can access accurate and timely insights to drive strategic actions.


#### Data Exposition
<!-- Preciser maybe pour des consomateurs internes -->
In addition to visualizing data, the data exposition function of the data platform provides business intelligence data to other systems through machine-to-machine solutions. This is accomplished by developing APIs using Python Flask. These APIs facilitate seamless data exchange and integration across various systems within the organization, ensuring that all relevant data points are accessible and actionable. To ensure scalability and reliability, these APIs will be hosted on Azure Kubernetes Service (AKS) as the Data Applications.
